{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPATDwlV5W4rOmWWB4Z6ecW"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"lizHaECXK-6W","executionInfo":{"status":"ok","timestamp":1735022627193,"user_tz":-540,"elapsed":7600,"user":{"displayName":"Ken Nakatsukasa","userId":"03241388911823517185"}}},"outputs":[],"source":["import tensorflow as tf\n","from tensorflow.keras import layers\n","import numpy as np\n","import matplotlib.pyplot as plt"]},{"cell_type":"code","source":["# Define the generator\n","def build_generator(NOISE_DIM):\n","    model = tf.keras.Sequential([\n","        layers.Dense(7*7*256, use_bias=False, input_shape=(NOISE_DIM,)),\n","        layers.BatchNormalization(),\n","        layers.LeakyReLU(),\n","        layers.Reshape((7, 7, 256)),\n","        layers.Conv2DTranspose(128, (5, 5), strides=(1, 1), padding='same', use_bias=False),\n","        layers.BatchNormalization(),\n","        layers.LeakyReLU(),\n","        layers.Conv2DTranspose(64, (5, 5), strides=(2, 2), padding='same', use_bias=False),\n","        layers.BatchNormalization(),\n","        layers.LeakyReLU(),\n","        layers.Conv2DTranspose(1, (5, 5), strides=(2, 2), padding='same', use_bias=False, activation='tanh'), # use tanh because the image is scaled to -1 to 1\n","        ])\n","    return model  # output shape is (batch, 28, 28, 1) same as train data shape."],"metadata":{"id":"C9t7kLIqauHN","executionInfo":{"status":"ok","timestamp":1735028709980,"user_tz":-540,"elapsed":262,"user":{"displayName":"Ken Nakatsukasa","userId":"03241388911823517185"}}},"execution_count":29,"outputs":[]},{"cell_type":"code","source":["# Define the discriminator\n","def build_discriminator():\n","    model = tf.keras.Sequential([\n","        layers.Conv2D(64, (5, 5), strides=(2, 2), padding='same', input_shape=[28, 28, 1]),\n","        layers.LeakyReLU(),\n","        layers.Dropout(0.3),\n","        layers.Conv2D(128, (5, 5), strides=(2, 2), padding='same'),\n","        layers.LeakyReLU(),\n","        layers.Dropout(0.3),\n","        layers.Flatten(),\n","        layers.Dense(1), # you could choose sigmoid, but not needed since the output will eventually be around 0 to 1.\n","        # If using sigmoid, set from_logits=False in loss function.\n","        ])\n","    return model"],"metadata":{"id":"ROn0F2gicKKP","executionInfo":{"status":"ok","timestamp":1735028713859,"user_tz":-540,"elapsed":275,"user":{"displayName":"Ken Nakatsukasa","userId":"03241388911823517185"}}},"execution_count":30,"outputs":[]},{"cell_type":"code","source":["class GAN:\n","    def __init__(self, latent_dim):\n","        self.latent_dim = latent_dim\n","        self.generator = build_generator(latent_dim)\n","        self.discriminator = build_discriminator()\n","\n","        # Define loss functions\n","        self.cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n","\n","        # Define optimizers\n","        self.generator_optimizer = tf.keras.optimizers.Adam(1e-4)\n","        self.discriminator_optimizer = tf.keras.optimizers.Adam(1e-4)\n","\n","    def generator_loss(self, fake_output):\n","        return self.cross_entropy(tf.ones_like(fake_output), fake_output)\n","\n","    def discriminator_loss(self, real_output, fake_output):\n","        real_loss = self.cross_entropy(tf.ones_like(real_output), real_output)\n","        fake_loss = self.cross_entropy(tf.zeros_like(fake_output), fake_output)\n","        return real_loss + fake_loss\n","\n","    @tf.function  # optional but will improve runtime speed.\n","    def train_step(self, images):\n","        batch_size = tf.shape(images)[0]\n","\n","        # Generate random noise\n","        noise = tf.random.normal([batch_size, self.latent_dim])\n","\n","        with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n","            # Generate fake images\n","            generated_images = self.generator(noise, training=True)\n","\n","            # Get discriminator decisions\n","            real_output = self.discriminator(images, training=True)\n","            fake_output = self.discriminator(generated_images, training=True)\n","\n","            # Calculate losses\n","            gen_loss = self.generator_loss(fake_output)\n","            disc_loss = self.discriminator_loss(real_output, fake_output)\n","\n","        # Calculate gradients\n","        gen_gradients = gen_tape.gradient(gen_loss, self.generator.trainable_variables)\n","        disc_gradients = disc_tape.gradient(disc_loss, self.discriminator.trainable_variables)\n","\n","        # Apply gradients\n","        self.generator_optimizer.apply_gradients(zip(gen_gradients, self.generator.trainable_variables))\n","        self.discriminator_optimizer.apply_gradients(zip(disc_gradients, self.discriminator.trainable_variables))\n","\n","        return gen_loss, disc_loss\n","\n","    def generate_and_save_images(self, epoch, test_input):\n","        predictions = self.generator(test_input, training=False)\n","\n","        fig = plt.figure(figsize=(4, 4))\n","        for i in range(predictions.shape[0]):\n","            plt.subplot(4, 4, i+1)\n","            plt.imshow(predictions[i, :, :, 0] * 0.5 + 0.5, cmap='gray')\n","            plt.axis('off')\n","\n","        #plt.savefig(f'image_at_epoch_{epoch:04d}.png')\n","        plt.close()"],"metadata":{"id":"mlxwDkwUfTp9","executionInfo":{"status":"ok","timestamp":1735029192711,"user_tz":-540,"elapsed":267,"user":{"displayName":"Ken Nakatsukasa","userId":"03241388911823517185"}}},"execution_count":40,"outputs":[]},{"cell_type":"code","source":["(train_images, _), (_, _) = tf.keras.datasets.mnist.load_data()\n","train_images = train_images.reshape(train_images.shape[0], 28, 28, 1).astype('float32')\n","train_images = (train_images - 127.5) / 127.5  # Normalize images to [-1, 1]"],"metadata":{"id":"eJSz8sZzh328","executionInfo":{"status":"ok","timestamp":1735028716841,"user_tz":-540,"elapsed":889,"user":{"displayName":"Ken Nakatsukasa","userId":"03241388911823517185"}}},"execution_count":32,"outputs":[]},{"cell_type":"code","source":["# Set training parameters\n","BUFFER_SIZE = 2000\n","BATCH_SIZE = 32\n","EPOCHS = 50\n","noise_dim = 100\n","num_examples_to_generate = 4"],"metadata":{"id":"-rk2nqEkvSX3","executionInfo":{"status":"ok","timestamp":1735028718496,"user_tz":-540,"elapsed":275,"user":{"displayName":"Ken Nakatsukasa","userId":"03241388911823517185"}}},"execution_count":33,"outputs":[]},{"cell_type":"code","source":["train_dataset = tf.data.Dataset.from_tensor_slices(train_images[:BUFFER_SIZE])  # Converts the train_images array into a TensorFlow Dataset object.\n","train_dataset = train_dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)"],"metadata":{"id":"BRe99obNh-_J","executionInfo":{"status":"ok","timestamp":1735029222808,"user_tz":-540,"elapsed":262,"user":{"displayName":"Ken Nakatsukasa","userId":"03241388911823517185"}}},"execution_count":42,"outputs":[]},{"cell_type":"code","source":["gan = GAN(noise_dim)"],"metadata":{"id":"si8iK2VrwC5Z","executionInfo":{"status":"ok","timestamp":1735028720748,"user_tz":-540,"elapsed":240,"user":{"displayName":"Ken Nakatsukasa","userId":"03241388911823517185"}}},"execution_count":35,"outputs":[]},{"cell_type":"code","source":["# Create seed for generating images\n","seed = tf.random.normal([num_examples_to_generate, noise_dim])"],"metadata":{"id":"PU5arzU2xBwS","executionInfo":{"status":"ok","timestamp":1735028740723,"user_tz":-540,"elapsed":243,"user":{"displayName":"Ken Nakatsukasa","userId":"03241388911823517185"}}},"execution_count":36,"outputs":[]},{"cell_type":"code","source":["seed.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nL464vU1xY_l","executionInfo":{"status":"ok","timestamp":1735028760058,"user_tz":-540,"elapsed":243,"user":{"displayName":"Ken Nakatsukasa","userId":"03241388911823517185"}},"outputId":"f57e3fc1-3085-4a20-de0d-cc02dd537aac"},"execution_count":38,"outputs":[{"output_type":"execute_result","data":{"text/plain":["TensorShape([4, 100])"]},"metadata":{},"execution_count":38}]},{"cell_type":"code","source":["# Training loop\n","for epoch in range(EPOCHS):\n","    for image_batch in train_dataset:  # Train for each batch.\n","        gen_loss, disc_loss = gan.train_step(image_batch)\n","\n","    # Generate example images every 10 epochs\n","    if (epoch + 1) % 10 == 0:\n","        gan.generate_and_save_images(epoch + 1, seed)\n","        print(f'Epoch {epoch+1}, Gen Loss: {gen_loss:.4f}, Disc Loss: {disc_loss:.4f}')\n"],"metadata":{"id":"bxWGPj7JxcLo"},"execution_count":null,"outputs":[]}]}